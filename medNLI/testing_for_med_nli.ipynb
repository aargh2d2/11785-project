{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbasecondab8a2a0b1ed86479d9f777b0a8cf633d1",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  In the ED, initial VS revealed T 98.9, HR 73, ...   \n",
       "1  In the ED, initial VS revealed T 98.9, HR 73, ...   \n",
       "2  In the ED, initial VS revealed T 98.9, HR 73, ...   \n",
       "3  63 year old woman with known history of hyperc...   \n",
       "4  63 year old woman with known history of hyperc...   \n",
       "\n",
       "                                   sentence2     gold_label  \n",
       "0     The patient is hemodynamically stable      entailment  \n",
       "1   The patient is hemodynamically unstable.  contradiction  \n",
       "2                    The patient is in pain.        neutral  \n",
       "3                   the patient was in a MVC     entailment  \n",
       "4         the patient has no medical history  contradiction  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>gold_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In the ED, initial VS revealed T 98.9, HR 73, ...</td>\n      <td>The patient is hemodynamically stable</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In the ED, initial VS revealed T 98.9, HR 73, ...</td>\n      <td>The patient is hemodynamically unstable.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>In the ED, initial VS revealed T 98.9, HR 73, ...</td>\n      <td>The patient is in pain.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>63 year old woman with known history of hyperc...</td>\n      <td>the patient was in a MVC</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>63 year old woman with known history of hyperc...</td>\n      <td>the patient has no medical history</td>\n      <td>contradiction</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "def load_jsonl(jsonl_fpath):\n",
    "    with open(jsonl_fpath) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    df = pd.DataFrame(lines)\n",
    "    df.columns = ['json_element']\n",
    "    df['json_element'].apply(json.loads)\n",
    "    df = pd.json_normalize(df['json_element'].apply(json.loads))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_jsonl('../data/mednli-1.0.0/mli_test_v1.jsonl')\n",
    "df = df[['sentence1','sentence2','gold_label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['contradiction', 'entailment', 'neutral']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "classes = list(np.unique(df['gold_label']))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 74%\n",
      " 26%\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-655c1569509f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" {int(round(result[c] * 100))}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# if true_label == 'entailment':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# elif label == 'contradiction':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", return_dict=True)\n",
    "\n",
    "'''\n",
    "The goal of the task is to classify a given premise-hypothesis pair into one of the three classes: entailment, contradiction, or neutral.\n",
    "'''\n",
    "\n",
    "classes = list(np.unique(df['gold_label']))\n",
    "\n",
    "for i in range(len(df)):\n",
    "    premise = df.iloc[i]['sentence1']\n",
    "    hypothesis = df.iloc[i]['sentence2']\n",
    "\n",
    "    true_label = df.iloc[i]['gold_label']\n",
    "        \n",
    "    tokenized_sequences = tokenizer(premise, hypothesis, return_tensors=\"pt\")\n",
    "\n",
    "    logits = model(**tokenized_sequences).logits\n",
    "    result = torch.softmax(logits, dim=1).tolist()[0]\n",
    "\n",
    "    for c in range(len(classes)):\n",
    "        print(f\"{classes[c]}: {int(round(result[c] * 100))}%\")\n",
    "        # if true_label == 'entailment':\n",
    "        # elif label == 'contradiction':\n",
    "        # elif label == 'neutral':\n",
    "\n",
    "    # Should not be contradiction\n",
    "    # for c in range(len(classes)):\n",
    "    #     print(f\"{classes[c]}: {int(round(not_paraphrase_results[c] * 100))}%\")\n",
    "    # # Should be neutral\n",
    "    # for c in range(len(classes)):\n",
    "    #     print(f\"{classes[c]}: {int(round(not_paraphrase_results[c] * 100))}%\")\n",
    "\n",
    "    \n",
    "# paraphrase_classification_logits = model(**paraphrase).logits\n",
    "# not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "# paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "# not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "Should be entailment\n",
    "for c in range(len(classes)):\n",
    "    print(f\"{classes[c]}: {int(round(paraphrase_results[c] * 100))}%\")\n",
    "# Should not be contradiction\n",
    "for c in range(len(classes)):\n",
    "    print(f\"{classes[c]}: {int(round(not_paraphrase_results[c] * 100))}%\")\n",
    "# Should be neutral\n",
    "for c in range(len(classes)):\n",
    "    print(f\"{classes[c]}: {int(round(not_paraphrase_results[c] * 100))}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}